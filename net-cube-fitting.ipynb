{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Steps in Inverse Rendering of Geometry\n",
    "======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on how to use our method for shape optimization with differentiable rendering. It uses `nvdiffrast` for the differentiable rendering part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cupy as cp\n",
    "\n",
    "from tqdm import trange\n",
    "from scripts.load_xml import load_scene\n",
    "from scripts.geometry import (\n",
    "    compute_vertex_normals, \n",
    "    compute_face_normals,\n",
    ")\n",
    "from scripts.render import NVDRenderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading the scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scene\n",
    "filepath = os.path.join(os.getcwd(), \"scenes\", \"centered\", \"centered.xml\")\n",
    "scene_params = load_scene(filepath)\n",
    "\n",
    "# Load reference shape\n",
    "v_ref = scene_params[\"mesh-target\"][\"vertices\"]\n",
    "n_ref = scene_params[\"mesh-target\"][\"normals\"]\n",
    "f_ref = scene_params[\"mesh-target\"][\"faces\"]\n",
    "\n",
    "v_ref, n_ref, f_ref = [f.contiguous() for f in [v_ref, n_ref, f_ref]]\n",
    "# Load source shape\n",
    "#v = scene_params[\"mesh-source\"][\"vertices\"]\n",
    "#f = scene_params[\"mesh-source\"][\"faces\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rendering references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to setup a differentiable rendering pipeline. Here, we use an implementation based on `nvdiffrast`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize it once, so it loads the camera data, the environment map and precomputes the shading model, using spherical harmonics in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the renderer\n",
    "renderer = NVDRenderer(scene_params, shading=True, boost=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render the target shape to use as a reference for the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_imgs = renderer.render(v_ref, n_ref, f_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of these references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow((ref_imgs[7,...,:-1].clip(0,1).pow(1/2.2)).cpu().numpy(), origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameterizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to setup the optimization. First, let us import what we need. We need an optimizer, `AdamUniform`, and the functions that allow us to convert back and forth between vertex positions and their parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000 # Number of optimization steps\n",
    "step_size = 3e-2 # Step size\n",
    "lambda_ = 19 # Hyperparameter lambda of our method, used to compute the matrix (I + lambda_*L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parameterize our shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.mesh.ellipsoid import Ellipsoid\n",
    "#from src.mesh.geoid import Geoid\n",
    "from src.mesh.cube import NetCube\n",
    "from argparse import Namespace\n",
    "\n",
    "opt = Namespace(\n",
    "    n = 82,\n",
    "    nfc=32, \n",
    "    min_nfc=32, \n",
    "    ker_size=3,\n",
    "    num_layer=5,\n",
    "    stride=1,\n",
    "    padd_size=0,\n",
    "    nc_im=3,\n",
    ")\n",
    "\n",
    "n = 128\n",
    "cube = NetCube(n, opt, kernel=7, sigma=2).cuda()\n",
    "\n",
    "#radii = torch.rand(3, device=v_ref.device, requires_grad=True)\n",
    "v, f = cube()\n",
    "# n_ref = compute_face_normals(v_ref, f_ref)\n",
    "[f.shape for f in [v, f]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our optimizer, `AdamUniform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from largesteps.optimize import AdamUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u.requires_grad = True\n",
    "\n",
    "#opt = AdamUniform(cube.parameters(), step_size)\n",
    "opt = torch.optim.Adam(cube.parameters(), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that is returned in the end, contains useful information for debug/analysis\n",
    "v_steps = torch.zeros((steps+1, *v.shape))#, device='cuda')\n",
    "losses = torch.zeros(steps+1)#, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run our optimization. The only difference with \"regular\" optimization here is the call to `from_differential` in the loop body, that converts the parameterization to vertex coordinates. The rest of the optimization pipeline is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "\n",
    "# Optimization loop\n",
    "for it in trange(steps):\n",
    "    #v, f = ellipsoid(torch.sigmoid(radii) * 2.)\n",
    "    #v, f = geoid()\n",
    "    v, f = cube()\n",
    "    # Recompute vertex normals\n",
    "    face_normals = compute_face_normals(v, f)\n",
    "    n = compute_vertex_normals(v, f, face_normals)\n",
    "\n",
    "    # Render images\n",
    "    v, n, f = [f.contiguous()  for f in [v, n, f]]\n",
    "    opt_imgs = renderer.render(v, n, f)\n",
    "\n",
    "    # Compute L1 image loss\n",
    "    loss = (opt_imgs - ref_imgs).abs().mean()\n",
    "\n",
    "    # Record optimization state for later processing\n",
    "    with torch.no_grad():\n",
    "        losses[it] = loss.cpu()\n",
    "        v_steps[it] = v.cpu()\n",
    "\n",
    "    # Backpropagate\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Render images\n",
    "    opt_imgs = renderer.render(v, n, f)\n",
    "    # Compute L1 image loss\n",
    "    loss = (opt_imgs - ref_imgs).abs().mean()\n",
    "    losses[-1] = loss\n",
    "    v_steps[-1] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshplot import plot\n",
    "from ipywidgets import interact\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_numpy = v_steps.cpu().numpy()\n",
    "f_numpy = f.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shading_params = {\n",
    "    \"width\": 600, \"height\": 600,\n",
    "    \"antialias\": True,\n",
    "    \"colormap\": \"viridis\",\n",
    "    \"wireframe\": True, \"wire_width\": 0.03, \"wire_color\": \"black\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the mesh across iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(it=(0, steps-1))\n",
    "def plot_verts(it):\n",
    "    plot(v_numpy[it], f_numpy, shading=shading_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conjure] *",
   "language": "python",
   "name": "conda-env-conjure-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
